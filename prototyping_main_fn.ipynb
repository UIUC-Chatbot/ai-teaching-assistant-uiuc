{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../data-generator\")\n",
    "sys.path.append(\"../info-retrieval\")\n",
    "import contriever.contriever_final # Asmita's contriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_QUESTION = 'What are the inputs and outputs of a Gray code counter?'\n",
    "NUM_ANSWERS_GENERATED = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Asmita's Contriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_contriever = contriever.contriever_final.ContrieverCB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_contriever.generate_embeddings(\"../data-generator/prompt engineering/gpt-3 semantic search/1_top_quality.json\")\n",
    "contriever_contexts = my_contriever.retrieve_topk(\"What are the inputs and outputs of a Gray code counter?\", path_to_json = \"../data-generator/split_textbook/paragraphs.json\", k = NUM_ANSWERS_GENERATED)\n",
    "\n",
    "top_context_list = list(contriever_contexts.values())\n",
    "for i, context in enumerate(top_context_list):\n",
    "    res = []\n",
    "    for sub in context:\n",
    "        res.append(sub.replace(\"\\n\", \"\"))\n",
    "    top_context_list[i] = \"\".join(res)\n",
    "print('\\n\\n'.join(top_context_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear cuda memory\n",
    "import gc\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, OPTForCausalLM\n",
    "import torch\n",
    "\n",
    "model = OPTForCausalLM.from_pretrained(\"facebook/opt-1.3b\") # opt-350m\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"facebook/opt-1.3b\")\n",
    "# model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate\n",
    "response_list = []\n",
    "for i in range(NUM_ANSWERS_GENERATED):\n",
    "  prompt = \"Please answer this person's question accurately, clearly and concicely. Context: \" + top_context_list[i] + '\\n' + \"Question: \" + USER_QUESTION + '\\n' + \"Answer: \"\n",
    "  inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "  model = model.to(\"cuda\")\n",
    "  \n",
    "  generate_ids = model.generate(inputs.input_ids, max_length=345, do_sample=True, top_k=50, top_p=0.95, temperature=0.95, num_return_sequences=1, repetition_penalty=1.2, length_penalty=1.2, pad_token_id=tokenizer.eos_token_id)\n",
    "  response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "  opt_answer = response.split(\"Answer:\")[1]\n",
    "  response_list.append(opt_answer)\n",
    "  \n",
    "print('\\n---------------------------------NEXT---------------------------------\\n'.join(response_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n---------------------------------NEXT---------------------------------\\n'.join(response_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReRanking (MS-Marco Cross Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "assert len([USER_QUESTION] * NUM_ANSWERS_GENERATED ) == len(response_list)\n",
    "\n",
    "features = tokenizer([USER_QUESTION] * NUM_ANSWERS_GENERATED, response_list,  padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    scores = model(**features).logits\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working!! Using Jerome's Doc-Query\n",
    "\n",
    "todo: figure out how to keep document in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Quotes from textbook: \n",
      "No, :  3.0541425076080486e-05\n",
      "No, :  3.6431545140658272e-06\n",
      "we do not have to restrict ourselves to non-negative numbers. :  9.00167478334879e-08\n",
      "This range of integers falls within the representable range :  4.4374765195698274e-08\n",
      "falls within the representable range for N-bit 2â€™s complement, :  2.578081037540869e-08\n"
     ]
    }
   ],
   "source": [
    "from docquery import document, pipeline\n",
    "import json\n",
    "import re\n",
    "import poppler\n",
    "\n",
    "\"\"\" Warning requires lots of memory and lots of dependencies.\"\"\"\n",
    "\n",
    "USER_QUESTION = \"What is the difference between a synchronous and asynchronous counter?\"\n",
    "NUM_ANSWERS_GENERATED = 5\n",
    "\n",
    "# Call the DocQuery class\n",
    "pipeline = pipeline('document-question-answering')\n",
    "doc = document.load_document(\"../data-generator/notes/Student_Notes_short.pdf\")\n",
    "# doc = document.load_document(\"../data-generator/notes/Student Notes.pdf\")\n",
    "answer = pipeline(question=USER_QUESTION, **doc.context, top_k=NUM_ANSWERS_GENERATED)\n",
    "# print(answer)\n",
    "print(\"Quotes from textbook: \")\n",
    "for item in answer:\n",
    "  print(item['answer'], \": \", item['score']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quotes from textbook: \n",
      "No, :  3.0541425076080486e-05\n",
      "No, :  3.6431545140658272e-06\n",
      "we do not have to restrict ourselves to non-negative numbers. :  9.00167478334879e-08\n",
      "This range of integers falls within the representable range :  4.4374765195698274e-08\n",
      "falls within the representable range for N-bit 2â€™s complement, :  2.578081037540869e-08\n"
     ]
    }
   ],
   "source": [
    "answer = pipeline(question=USER_QUESTION, **doc.context, top_k=NUM_ANSWERS_GENERATED)\n",
    "# print(answer)\n",
    "print(\"Quotes from textbook: \")\n",
    "for item in answer:\n",
    "  print(item['answer'], \": \", item['score']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try running this ðŸ‘‡ This works running our main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import main\n",
    "\n",
    "ta = main.TA_Pipeline()\n",
    "\n",
    "USER_QUESTION = \"user_question = 'What are the inputs and outputs of a Gray code counter?'\"\n",
    "NUM_ANSWERS_GENERATED = 5\n",
    "\n",
    "top_context_list = ta.contriever(user_question=USER_QUESTION, num_answers_generated=NUM_ANSWERS_GENERATED)\n",
    "generated_answers_list = ta.OPT(top_context_list)\n",
    "scores = ta.re_ranking_ms_marco(generated_answers_list)\n",
    "index_of_best_answer = torch.argmax(scores) # get best answer\n",
    "print(\"Best answer ðŸ‘‡\\n\", generated_answers_list[index_of_best_answer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general_has_everything",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b69eb772e953fa94c7e6a13e20e5676ab72e6ff0f5efef62f5cddb3889daa1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
